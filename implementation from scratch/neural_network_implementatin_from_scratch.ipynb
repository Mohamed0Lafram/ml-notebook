{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14415c-7a48-489a-a0ab-9d29a0d9114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f357b39-4dbb-4dc4-8f0e-ce5f194140b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(A):\n",
    "    return A * (1 - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f3247-d34f-42a8-a98f-ed7e468c31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, X, Y, layer_sizes):\n",
    "        self.X = X  # shape (m, n_features)\n",
    "        self.Y = Y.reshape(1, -1)  # shape (1, m)\n",
    "        self.layer_sizes = layer_sizes  # [n_input, n1, n2, ..., n_output]\n",
    "        self.L = len(layer_sizes) - 1  # number of layers (excluding input)\n",
    "\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        self.cache = {}\n",
    "\n",
    "        # Weight initialization\n",
    "        for l in range(1, len(layer_sizes)):\n",
    "            self.weights[l] = np.random.randn(layer_sizes[l], layer_sizes[l - 1]) * 0.01\n",
    "            self.biases[l] = np.zeros((layer_sizes[l], 1))\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        A = self.X.T  # shape (n_features, m)\n",
    "        self.cache['A0'] = A\n",
    "\n",
    "        for l in range(1, self.L + 1):\n",
    "            Z = self.weights[l] @ A + self.biases[l]\n",
    "            A = sigmoid(Z)\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "\n",
    "        return A  # output layer activation\n",
    "\n",
    "    def compute_cost(self, AL):\n",
    "        m = self.Y.shape[1]\n",
    "        cost = -1 / m * np.sum(self.Y * np.log(AL) + (1 - self.Y) * np.log(1 - AL))\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    def backward_propagation(self):\n",
    "        grads = {}\n",
    "        m = self.X.shape[0]\n",
    "        Y = self.Y\n",
    "        L = self.L\n",
    "\n",
    "        AL = self.cache[f'A{L}']\n",
    "        dAL = AL - Y  # derivative of binary cross-entropy loss\n",
    "\n",
    "        for l in reversed(range(1, L + 1)):\n",
    "            A_prev = self.cache[f'A{l - 1}']\n",
    "            Z = self.cache[f'Z{l}']\n",
    "            dZ = dAL * sigmoid_derivative(AL) if l == L else grads[f'dA{l}'] * sigmoid_derivative(self.cache[f'A{l}'])\n",
    "            grads[f'dW{l}'] = 1 / m * dZ @ A_prev.T\n",
    "            grads[f'db{l}'] = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "            grads[f'dA{l - 1}'] = self.weights[l].T @ dZ\n",
    "            dAL = grads[f'dA{l - 1}']\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.weights[l] -= learning_rate * grads[f'dW{l}']\n",
    "            self.biases[l] -= learning_rate * grads[f'db{l}']\n",
    "\n",
    "    def fit(self, iterations=1000, learning_rate=0.1, verbose=True):\n",
    "        for i in range(iterations):\n",
    "            AL = self.forward_propagation()\n",
    "            cost = self.compute_cost(AL)\n",
    "            grads = self.backward_propagation()\n",
    "            self.update_parameters(grads, learning_rate)\n",
    "\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"Iteration {i} - Cost: {cost:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        A = X.T\n",
    "        for l in range(1, self.L + 1):\n",
    "            Z = self.weights[l] @ A + self.biases[l]\n",
    "            A = sigmoid(Z)\n",
    "        return (A > 0.5).astype(int)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions.flatten() == Y.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e6b8b-de91-4451-845b-2b52045896dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données de test (ex: XOR ou toute donnée binaire)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "Y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# Création et entraînement du modèle\n",
    "nn = NeuralNetwork(X, Y, layer_sizes=[2, 4, 1])\n",
    "nn.fit(iterations=10000, learning_rate=0.5)\n",
    "\n",
    "# Prédictions\n",
    "print(\"Prédictions :\", nn.predict(X).T)\n",
    "print(\"Accuracy :\", nn.accuracy(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870f7ff-311b-4e05-b474-1b014c5928fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0afde8-73cc-4069-8712-daaf40e4fca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550da1b6-d3bc-4a48-bb34-311ce0f04f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
